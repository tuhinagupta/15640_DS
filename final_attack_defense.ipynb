{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_attack_defense.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuhinagupta/15640_DS/blob/master/final_attack_defense.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIzYcyoUific",
        "outputId": "3cef99f3-d550-4f0b-9c49-518dd2777a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoigwIy2kJ8c"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "\n",
        "import pandas as pd\n",
        "from keras import regularizers\n",
        "import torch\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWHcjN3_Z-Wy"
      },
      "source": [
        "LOADING SAVED MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u6z3ooPZ9cM"
      },
      "source": [
        "base_classifier = \"saved_models/base_classifier_resnet.pth\"\n",
        "attack_model = \"saved_models/attack_model_vgg.pth\"\n",
        "fgsm_trained_model = \"saved_models/fgsm_trained_resnet.pth\"\n",
        "pgd_trained_model = \"saved_models/pgd_trained_resnet.pth\"\n",
        "fgsm_pgd_trained_model = \"saved_models/fgsm_pgd_trained_resnet.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbpdxsN9mPpi",
        "outputId": "3c305b3f-b1c0-4c83-8d56-5e771f43d334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' Data loading and augmentation '''\n",
        "transform = torchvision.transforms.Compose([\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   # [torchvision.transforms.Resize(size = (224,224)),\n",
        "\n",
        "transform1 = torchvision.transforms.Compose([\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   # [torchvision.transforms.Resize(size = (224,224)),\n",
        "     \n",
        "transform2 = torchvision.transforms.Compose([\n",
        "     torchvision.transforms.RandomHorizontalFlip(1),\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   # [torchvision.transforms.Resize(size = (224,224)),\n",
        "\n",
        "trainset1 = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform1)\n",
        "trainset2 = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform2)\n",
        "\n",
        "trainset = torch.utils.data.ConcatDataset([trainset1, trainset2])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
        "                                         shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwTMhGBLQMuL"
      },
      "source": [
        "CLASSIFIER /DEFENSE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPgVC7GsFYfM"
      },
      "source": [
        "'''Basic building block 1 for ResNet18 model'''\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self,channel_size1, channel_size2, stride1=1, stride2=2):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_size1, channel_size2, kernel_size=3, stride=stride1, padding=1, bias=False)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(channel_size2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2 = nn.Conv2d(channel_size2, channel_size2, kernel_size=3, stride=stride1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_size2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.shortcut = nn.Conv2d(channel_size1, channel_size2, kernel_size=1, stride=stride1, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22iZ1m_BJiET"
      },
      "source": [
        "'''Basic building block 2 for ResNet18 model'''\n",
        "class BasicBlock2(nn.Module):\n",
        "\n",
        "    def __init__(self, channel_size, stride=1):\n",
        "        super(BasicBlock2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_size, channel_size, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.shortcut = nn.Conv2d(channel_size, channel_size, kernel_size=1, stride=stride, bias=False)\n",
        "        self.conv2 = nn.Conv2d(channel_size, channel_size, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_size, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN9rachuJmSS"
      },
      "source": [
        "''' ResNet18 model architecture for a 32x32 input image '''\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_feats, hidden_sizes, num_classes, feat_dim=10):\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "\n",
        "        self.hidden_sizes = [num_feats] + hidden_sizes + [num_classes]\n",
        "        \n",
        "        self.layers = []\n",
        "        for idx, channel_size in enumerate(hidden_sizes):\n",
        "            if idx==0:\n",
        "              self.layers.append(nn.Conv2d(in_channels=self.hidden_sizes[idx], \n",
        "                                            out_channels=self.hidden_sizes[idx+1], \n",
        "                                            kernel_size=3, stride=1, padding = 1, bias=False))\n",
        "              self.layers.append(nn.BatchNorm2d(num_features=self.hidden_sizes[idx+1],eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
        "              self.layers.append(nn.ReLU(inplace=True))\n",
        "              \n",
        "              self.layers.append(nn.MaxPool2d(3, stride=2, padding=1, dilation=1, ceil_mode=False))\n",
        "              # 2 times\n",
        "              self.layers.append(nn.Sequential(BasicBlock2(channel_size = channel_size),\n",
        "                                                BasicBlock2(channel_size = channel_size)))\n",
        "            else:\n",
        "              self.layers.append(BasicBlock(channel_size1=self.hidden_sizes[idx], channel_size2=self.hidden_sizes[idx+1]))\n",
        "              self.layers.append(BasicBlock2(channel_size=channel_size))\n",
        "\n",
        "        self.layers.append(nn.AdaptiveAvgPool2d(output_size=(1,1)))\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "        \n",
        "        self.linear1 = nn.Linear(self.hidden_sizes[-2], self.hidden_sizes[-1], bias=False)\n",
        "    \n",
        "    def forward(self, x, evalMode=False):\n",
        "        output = x\n",
        "        output = self.layers(output)\n",
        "        \n",
        "        output = F.avg_pool2d(output, [output.size(2), output.size(3)], stride=1)\n",
        "        output = output.reshape(output.shape[0], output.shape[1])\n",
        "        \n",
        "        label_output = self.linear1(output)\n",
        "\n",
        "        return label_output\n",
        "\n",
        "# Convolutional and linear layer initializations \n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXDKOLvQXj7"
      },
      "source": [
        "ATTACK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHRN6WiCSPoH"
      },
      "source": [
        "''' VGG16 basic block for 2 blocks '''\n",
        "class BasicBlock2B(nn.Module):\n",
        "    def __init__(self,channel_size1, channel_size2, stride1=1):\n",
        "        super(BasicBlock2B, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_size1, channel_size2, kernel_size=3, stride=stride1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(channel_size2, channel_size2, kernel_size=3, stride=stride1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_size2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x), inplace=True)\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(out)\n",
        "        mp = nn.MaxPool2d(2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        out = self.bn1(mp(out))\n",
        "        dm = nn.Dropout(p=0.25)\n",
        "        output = dm(out)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk_O870ISTe1"
      },
      "source": [
        "''' VGG16 basic block for 3 blocks '''\n",
        "class BasicBlock3B(nn.Module):\n",
        "    def __init__(self,channel_size1, channel_size2, stride1=1):\n",
        "        super(BasicBlock3B, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_size1, channel_size2, kernel_size=3, stride=stride1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(channel_size2, channel_size2, kernel_size=3, stride=stride1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_size2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3 = nn.Conv2d(channel_size2, channel_size2, kernel_size=3, stride=stride1, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x), inplace=True)\n",
        "        out = F.relu(self.conv2(out), inplace=True)\n",
        "        out = F.relu(self.conv3(out), inplace=True)\n",
        "        mp = nn.MaxPool2d(2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        out = self.bn1(mp(out))\n",
        "        \n",
        "        dm = nn.Dropout(p=0.25)\n",
        "        output = dm(out)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZhKSYdjRBoy"
      },
      "source": [
        "''' VGG16 model architecture for 32x32 input image '''\n",
        "class Network_Attack(nn.Module):\n",
        "    def __init__(self, num_feats, hidden_sizes, num_classes, feat_dim=10):\n",
        "        super(Network_Attack, self).__init__()\n",
        "\n",
        "        self.hidden_sizes = [num_feats] + hidden_sizes + [num_classes]\n",
        "        \n",
        "        self.layers = []\n",
        "        for idx, channel_size in enumerate(hidden_sizes):\n",
        "          if idx == 0 or idx == 1:\n",
        "            self.layers.append(BasicBlock2B(channel_size1=self.hidden_sizes[idx], channel_size2=self.hidden_sizes[idx+1]))\n",
        "          if idx == 2:\n",
        "            self.layers.append(BasicBlock3B(channel_size1=self.hidden_sizes[idx], channel_size2=self.hidden_sizes[idx+1]))\n",
        "          if idx == 3:\n",
        "            self.layers.append(BasicBlock3B(channel_size1=self.hidden_sizes[idx], channel_size2=self.hidden_sizes[idx+1]))\n",
        "            self.layers.append(BasicBlock3B(channel_size1=self.hidden_sizes[idx+1], channel_size2=self.hidden_sizes[idx+1]))\n",
        "\n",
        "        self.layers.append(nn.AdaptiveAvgPool2d(output_size=(7,7)))\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "        \n",
        "        self.linear1 = nn.Linear(self.hidden_sizes[-2], self.hidden_sizes[-1], bias=False)\n",
        "        \n",
        "    \n",
        "    def forward(self, x, evalMode=False):\n",
        "        output = x\n",
        "        output = self.layers(output)\n",
        "        \n",
        "        output = F.avg_pool2d(output, [output.size(2), output.size(3)], stride=1)\n",
        "        output = output.reshape(output.shape[0], output.shape[1])\n",
        "        \n",
        "        label_output = self.linear1(output)\n",
        "\n",
        "        return label_output\n",
        "\n",
        "#Weights initializations\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbwB4ZMLRRyt"
      },
      "source": [
        "CLASSIFIER TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-jfYWVBEmXt"
      },
      "source": [
        "''' Model training '''\n",
        "def train(model, data_loader, test_loader, task='Classification'):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        avg_loss = 0.0\n",
        "        accuracy = 0.0\n",
        "        total = 0.0\n",
        "\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(feats)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            avg_loss += loss.item()\n",
        "            accuracy += torch.sum((torch.argmax(outputs,dim=1)==labels)).item()\n",
        "            total += len(labels)\n",
        "\n",
        "            if batch_num % 50 == 49:\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
        "                avg_loss = 0.0    \n",
        "                \n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "            del outputs \n",
        "\n",
        "        \n",
        "        print(\"Accuracy in train\", (accuracy/total)*100.0)\n",
        "        if task == 'Classification':\n",
        "            train_loss, train_acc = test_classify(model, data_loader)\n",
        "            val_loss, val_acc = test_classify(model, test_loader)\n",
        "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "                  format(train_loss, train_acc*100.0, val_loss, val_acc*100.0))\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "''' Function to return accuracy of model ''' \n",
        "def test_classify(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        outputs = model(feats).detach()\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        accuracy += torch.sum((torch.argmax(outputs,dim=1)==labels)).item()\n",
        "        total += len(labels)\n",
        "        test_loss.extend([loss.item()]*feats.size()[0])\n",
        "\n",
        "        del feats\n",
        "        del labels\n",
        "        del outputs\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(test_loss), accuracy/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYSUv8ImEmds"
      },
      "source": [
        "numEpochs = 32\n",
        "num_feats = 3\n",
        "\n",
        "learningRate = 0.15\n",
        "weightDecay = 1e-3\n",
        "\n",
        "hidden_sizes = [64, 128, 256, 512]\n",
        "num_classes = len(trainset1.classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "network = Network(num_feats, hidden_sizes, num_classes)\n",
        "network.apply(init_weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=0.85)   \n",
        "\n",
        "network.train()\n",
        "network.to(device)\n",
        "train(network, trainloader, testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7si2cK1e4Tq"
      },
      "source": [
        "ATTACK MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iu0sYm2etYC"
      },
      "source": [
        "numEpochs = 40\n",
        "num_feats = 3\n",
        "\n",
        "learningRate = 1e-3\n",
        "weightDecay = 5e-5\n",
        "\n",
        "hidden_sizes = [64, 128, 256, 512]\n",
        "num_classes = len(trainset1.classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "network = Network_Attack(num_feats, hidden_sizes, num_classes)\n",
        "network.apply(init_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(network.parameters(), lr = 1e-3, weight_decay=weightDecay)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=0.85)   \n",
        "\n",
        "network.train()\n",
        "network.to(device)\n",
        "train(network, trainloader, testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq4kun6d67MM"
      },
      "source": [
        "FGSM ATTACK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8ALC9No6aat"
      },
      "source": [
        "''' FGSM attack code '''\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon*sign_data_grad\n",
        "    # Adding clipping to maintain [-1,1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0CTn_TnNZVg"
      },
      "source": [
        "''' Generate adversarial images using FGSM attack '''\n",
        "def attack_images(model, test_loader, epsilon):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    p_images = []\n",
        "    p_label = []\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        feats.requires_grad = True\n",
        "\n",
        "        outputs = model(feats)\n",
        "        init_pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if init_pred != labels.item():\n",
        "            p_images.append(feats)\n",
        "            p_label.append(labels)\n",
        "            continue\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Call FGSM Attack\n",
        "        perturbed_data = fgsm_attack(feats, epsilon, feats.grad.data)\n",
        "        p_images.append(perturbed_data)\n",
        "        p_label.append(labels)\n",
        "           \n",
        "        del feats\n",
        "        del labels\n",
        "        del outputs\n",
        "\n",
        "    return p_images,p_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-5GNHPdZ33y"
      },
      "source": [
        "PGD ATTACK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpOujSxsTM2u"
      },
      "source": [
        "''' PGD attack adversarial images generation '''\n",
        "def pgd_attack(image,epsilon,data_grad,alpha,steps,random_start):\n",
        "\n",
        "  #make sure alpha*steps = epsilon\n",
        "  \n",
        "  adv_images = image\n",
        "  #random initialization \n",
        "  if random_start == True:\n",
        "    adv_images = adv_images + torch.empty_like(adv_images).uniform_(-epsilon,epsilon)\n",
        "    adv_images = torch.clamp(adv_images,0, 1)\n",
        "\n",
        "  for i in range(steps):\n",
        "    adv_images = adv_images + alpha*data_grad.sign()\n",
        "    delta = torch.clamp(adv_images - image,-epsilon,epsilon)\n",
        "    adv_images = torch.clamp(image + delta,0, 1)\n",
        "\n",
        "  return adv_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEast065TTee"
      },
      "source": [
        "''' Train the model using PGD-generated attack images '''\n",
        "def attack_images_pgd(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "### PGD attack values ###\n",
        "    epsilon = 0.2\n",
        "    random_start = True\n",
        "    alpha = 1/255\n",
        "    steps = 50\n",
        "    p_images = []\n",
        "    p_label = []\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        feats.requires_grad = True\n",
        "\n",
        "        outputs = model(feats)\n",
        "        init_pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if init_pred != labels.item():\n",
        "            p_images.append(feats)\n",
        "            p_label.append(labels)\n",
        "            continue\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Call PGD Attack\n",
        "        perturbed_data = pgd_attack(feats, epsilon, feats.grad.data, alpha, steps, random_start)\n",
        "        p_images.append(perturbed_data)\n",
        "        p_label.append(labels)\n",
        "           \n",
        "        del feats\n",
        "        del labels\n",
        "        del outputs\n",
        "\n",
        "    return p_images,p_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bifShRfWPEXg"
      },
      "source": [
        "ADVERSARIAL TRAINING (FGSM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmC4DlHf8kZa"
      },
      "source": [
        "def train_adv(model, data_loader, test_loader,epsilon, task='Classification'):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        avg_loss = 0.0\n",
        "        accuracy = 0.0\n",
        "        total = 0.0\n",
        "\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            feats.requires_grad = True\n",
        "\n",
        "            # for clean data\n",
        "            optimizer.zero_grad()            \n",
        "            outputs = model(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            accuracy += torch.sum((torch.argmax(outputs,dim=1)==labels)).item()\n",
        "            total += len(labels)\n",
        "\n",
        "            # for adversarial examples\n",
        "            feats = fgsm_attack(feats, epsilon, feats.grad.data)\n",
        "            optimizer.zero_grad()            \n",
        "            outputs = model(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            accuracy += torch.sum((torch.argmax(outputs,dim=1)==labels)).item()\n",
        "            total += len(labels)\n",
        "\n",
        "            if batch_num % 50 == 49:\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
        "                avg_loss = 0.0    \n",
        "                \n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "            del outputs \n",
        "\n",
        "        \n",
        "        print(\"Accuracy in train\", (accuracy/total)*100.0)\n",
        "        if task == 'Classification':\n",
        "            train_loss, train_acc = test_classify(model, data_loader)\n",
        "            val_loss, val_acc = test_classify(model, test_loader)\n",
        "            val_acc_attack = test_attack(model,test_loader,epsilon)\n",
        "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}\\tVal Accuracy (Attack): {:.4f}'.\n",
        "                  format(train_loss, train_acc*100.0, val_loss, val_acc*100.0, val_acc_attack*100))\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "''' Test the attack against the model '''\n",
        "def test_attack(model, test_loader, epsilon):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        feats.requires_grad = True\n",
        "\n",
        "        outputs = model(feats)\n",
        "        init_pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if init_pred != labels.item():\n",
        "            continue\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Call FGSM Attack\n",
        "        perturbed_data = fgsm_attack(feats, epsilon, feats.grad.data)\n",
        "        outputs = model(perturbed_data) \n",
        "        \n",
        "        # Check for success\n",
        "        final_pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if final_pred.item() == labels.item():\n",
        "            correct += 1\n",
        "           \n",
        "        del feats\n",
        "        del labels\n",
        "        del outputs\n",
        "    \n",
        "    # Calculate final accuracy for this epsilon\n",
        "    final_acc = correct/float(len(test_loader))\n",
        "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
        "\n",
        "    model.train()\n",
        "    return final_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZA_vdZuPcAx"
      },
      "source": [
        "numEpochs = 20\n",
        "num_feats = 3\n",
        "\n",
        "learningRate = 0.15\n",
        "weightDecay = 1e-3\n",
        "\n",
        "hidden_sizes = [64, 128, 256, 512]\n",
        "num_classes = len(trainset1.classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "network = torch.load(base_classifier)\n",
        "network.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=0.85)   \n",
        "\n",
        "network.train()\n",
        "train_adv(network, trainloader, testloader, 0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPc-V5JlcdTq"
      },
      "source": [
        "ADVERSARIAL TRAINING (PGD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxNHebEqcgM_"
      },
      "source": [
        "def train_adv(model, data_loader, test_loader,epsilon,alpha,steps,random_start, task='Classification'):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        avg_loss = 0.0\n",
        "        accuracy = 0.0\n",
        "        total = 0.0\n",
        "\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            feats.requires_grad = True\n",
        "\n",
        "            # for clean data\n",
        "            optimizer.zero_grad()            \n",
        "            outputs = model(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            accuracy += torch.sum((torch.argmax(outputs,dim=1)==labels)).item()\n",
        "            total += len(labels)\n",
        "\n",
        "            # for adversarial examples\n",
        "            feats = pgd_attack(feats, epsilon, feats.grad.data,alpha,steps,random_start)\n",
        "            optimizer.zero_grad()            \n",
        "            outputs = model(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            accuracy += torch.sum((torch.argmax(outputs,dim=1)==labels)).item()\n",
        "            total += len(labels)\n",
        "\n",
        "            if batch_num % 50 == 49:\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
        "                avg_loss = 0.0    \n",
        "                \n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "            del outputs #Can delete\n",
        "\n",
        "        \n",
        "        print(\"Accuracy in train\", (accuracy/total)*100.0)\n",
        "        if task == 'Classification':\n",
        "            print(\"In\")\n",
        "            train_loss, train_acc = test_classify(model, data_loader)\n",
        "            val_loss, val_acc = test_classify(model, test_loader)\n",
        "            val_acc_attack = test_attack(model,test_loader,epsilon,alpha,steps,random_start)\n",
        "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}\\tVal Accuracy (Attack): {:.4f}'.\n",
        "                  format(train_loss, train_acc*100.0, val_loss, val_acc*100.0, val_acc_attack*100))\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "def test_attack(model, test_loader, epsilon,alpha,steps,random_start):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        feats.requires_grad = True\n",
        "\n",
        "        outputs = model(feats)\n",
        "        init_pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if init_pred != labels.item():\n",
        "            continue\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        # Call PGD attack \n",
        "        perturbed_data = pgd_attack(feats, epsilon, feats.grad.data,alpha,steps,random_start)\n",
        "        outputs = model(perturbed_data) \n",
        "      \n",
        "        \n",
        "        # Check for success\n",
        "        final_pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if final_pred.item() == labels.item():\n",
        "            correct += 1\n",
        "           \n",
        "        del feats\n",
        "        del labels\n",
        "        del outputs\n",
        "    \n",
        "    # Calculate final accuracy for this epsilon\n",
        "    final_acc = correct/float(len(test_loader))\n",
        "    print(\"Epsilon: {} Alpha:{} \\tTest Accuracy = {} / {} = {}\".format(epsilon,alpha, correct, len(test_loader), final_acc))\n",
        "\n",
        "    model.train()\n",
        "    return final_acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nko8kCVcgat"
      },
      "source": [
        "defense_model = torch.load(fgsm_trained_model)\n",
        "numEpochs = 50\n",
        "num_feats = 3\n",
        "\n",
        "learningRate = 0.15\n",
        "weightDecay = 1e-3\n",
        "\n",
        "hidden_sizes = [64, 128, 256, 512]\n",
        "num_classes = len(trainset1.classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "defense_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(defense_model.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=0.85)   \n",
        "\n",
        "\n",
        "epsilon = 0.3\n",
        "random_start = True\n",
        "alpha = 2/255\n",
        "steps = 40\n",
        "train_adv(defense_model,trainloader, testloader, epsilon,alpha,steps,random_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFyTDZV9UIAz"
      },
      "source": [
        "FGSM ATTACK ON DIFFERENT MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1DSmDiX7Z0"
      },
      "source": [
        "def test_attack(model, attack_set):\n",
        "    model.eval()\n",
        "\n",
        "    device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    criterion = nn.CrossEntropyLoss()    \n",
        "\n",
        "    test_loss = []\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for i in range(len(attack_set[0])):\n",
        "\n",
        "        feats = attack_set[0][i]\n",
        "        labels = attack_set[1][i]\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        outputs = model(feats)\n",
        "        accuracy += torch.sum((torch.argmax(outputs,dim=1)==labels)).item()\n",
        "        total += len(labels)\n",
        "\n",
        "        del feats\n",
        "        del labels\n",
        "        del outputs\n",
        "\n",
        "    print(\"Accuracy\", accuracy/total)\n",
        "    return accuracy/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fxxvBmpSh_C"
      },
      "source": [
        "attack_model = torch.load(attack_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nvX9uQIT7T1"
      },
      "source": [
        "attack_images_fgsm = attack_images(attack_model,testloader, 0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC5Y5O3JZL5y",
        "outputId": "1dbd1d33-d62e-4b06-8a8d-e557d5764da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "defense_model = torch.load(base_classifier)\n",
        "test_attack(defense_model,attack_images_fgsm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.3744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2JMHnRCZO1w",
        "outputId": "0840fe5a-c5f7-4aa3-ecb2-b2f12e7eefcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "defense_model = torch.load(fgsm_trained_model)\n",
        "test_attack(defense_model,attack_images_fgsm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.7206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIwyX8PwbYij",
        "outputId": "4d57310c-45cb-4616-d24a-150f9c900ec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "defense_model = torch.load(pgd_trained_model)\n",
        "test_attack(defense_model,attack_images_fgsm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.7168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmRD9IUvdcJ2",
        "outputId": "038228f0-8fc7-41dd-8367-d4885fe3c485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "defense_model = torch.load(fgsm_pgd_trained_model)\n",
        "test_attack(defense_model,attack_images_fgsm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.7116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7116"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFnoSoemTvmD"
      },
      "source": [
        "PGD ATTACK ON DIFFERENT MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHMMHOCifR-8"
      },
      "source": [
        "attack_images_pgd = attack_images_pgd(attack_model,testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSIgY6TdfacQ",
        "outputId": "1be8303e-3266-4f55-c298-d9bfd83cf8af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "defense_model = torch.load(base_classifier)\n",
        "test_attack(defense_model,attack_images_pgd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.3129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW-oXiu8ftRm",
        "outputId": "009824ee-aaf2-485a-dbe8-aa5da60dbde2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "defense_model = torch.load(fgsm_trained_model)\n",
        "test_attack(defense_model,attack_images_pgd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.3758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imvo7a30qwfK",
        "outputId": "6fe6c3ac-77a1-4a41-a8ec-9c9f7329ed65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "defense_model = torch.load(pgd_trained_model)\n",
        "test_attack(defense_model,attack_images_pgd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.6181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEMN5NeJfyjr",
        "outputId": "649af91c-1b17-4f7d-dba3-4733f3778019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "defense_model = torch.load(fgsm_pgd_trained_model)\n",
        "test_attack(defense_model,attack_images_pgd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.6457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6457"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz3R-t-gtLfr"
      },
      "source": [
        "!cd ./gdrive/My\\ Drive/"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9OngrmItQnw",
        "outputId": "f0fbb7d7-f6b6-4e84-e76f-34a9663159cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  gdrive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0cyd1Cqtd0m",
        "outputId": "c99e8d06-97de-47d7-ce15-d40ff4d99423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git status"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "\n",
            "No commits yet\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mdata/\u001b[m\n",
            "\t\u001b[31mgdrive/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbtujVODtk89",
        "outputId": "4a39a3eb-cbe5-4c42-ad22-cf54317bf0bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  gdrive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7evEShJt3iw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}